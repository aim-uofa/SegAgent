<div align="center">

# ğŸ¯ SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by Imitating Human Annotator Trajectories

[Muzhi Zhu](https://scholar.google.com/citations?user=064gBH4AAAAJ&hl=en)<sup>1,2</sup>, &nbsp;
Yuzhuo Tian<sup>1</sup>, &nbsp;
[Hao Chen](https://stan-haochen.github.io/)<sup>1*</sup>, &nbsp;
Chunluan Zhou<sup>2</sup>, &nbsp;
Qingpei Guo<sup>2*</sup>, &nbsp;
Yang Liu<sup>1</sup>, &nbsp;
Ming Yang<sup>2</sup>, &nbsp;
[Chunhua Shen](https://cshen.github.io/)<sup>1*</sup>

<sup>1</sup>[Zhejiang University](https://www.zju.edu.cn/english/), &nbsp;
<sup>2</sup>[Ant Group](https://www.antgroup.com/en)

**CVPR2025**

[ğŸ“„ **Paper**](https://arxiv.org/abs/2503.08625)
</div>

## ğŸš€ Overview
<div align="center">
<img width="800" alt="SegAgent Framework" src="images/framework.png">
</div>

## ğŸ“– Description

Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities in understanding images but still struggle with pixel-level tasks like segmentation. SegAgent addresses this by introducing a novel **Human-Like Mask Annotation Task (HLMAT)**, enabling MLLMs to mimic the annotation trajectories of human experts using interactive segmentation tools.

SegAgent effectively leverages these annotation trajectories without requiring architectural modifications or additional implicit tokens. Our approach significantly enhances MLLMs' segmentation and mask refinement abilities, establishing a new paradigm for assessing fine-grained visual understanding and multi-step reasoning.




## ğŸš© Plan
<!-- - [ ] Release the weights. -->
- [ ] Release the weights.
- [ ] Release the inference code.
- [ ] Release the trajectory generation code and training scripts.
<!-- --- -->



## ğŸ› ï¸ Getting Started



## ğŸ« License

For academic usage, this project is licensed under [the 2-clause BSD License](LICENSE). For commercial inquiries, please contact [Chunhua Shen](mailto:chhshen@gmail.com).

## ğŸ–Šï¸ Citation

If you find this work helpful for your research, please cite:

```BibTeX
@article{zhu2025segagent,
  title={SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by Imitating Human Annotator Trajectories},
  author={Zhu, Muzhi and Tian, Yuzhuo and Chen, Hao and Zhou, Chunluan and Guo, Qingpei and Liu, Yang and Yang, Ming and Shen, Chunhua},
  journal={arXiv preprint arXiv:2503.08625},
  year={2025},
  url={https://arxiv.org/abs/2503.08625}
}